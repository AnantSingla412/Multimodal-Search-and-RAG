# ðŸš€ Building Multimodal Search and RAG  

## ðŸ“Œ Overview  
This project implements a **Multimodal Search & Retrieval-Augmented Generation (RAG) System**, enhancing LLMs by integrating **text, images, audio, and video** for better context-aware responses.  

## ðŸŽ¯ Key Features  
- **Contrastive Learning:** Trained multimodal models for modality-independent embeddings.  
- **Any-to-Any Retrieval:** Built a search system to retrieve relevant context across different data types.  
- **Multimodal RAG:** Integrated AI-powered reasoning over multimodal inputs.  
- **Structured Data Extraction:** Processed invoices, flowcharts, and images into structured outputs.  
- **Multi-Vector Recommendation:** Developed an AI-driven recommender system for cross-modal suggestions.  

## ðŸ“œ Reference  
This project is based on the **"Building Multimodal Search and RAG"** course, covering the technical aspects of implementing RAG with multimodal data. It explores **contrastive learning, multimodal retrieval, structured data extraction, and AI-driven recommendations** to enhance LLMs.  

 

